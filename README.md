# A Study on Deep Latent Representations of Stroke Lesion Imaging 

This repository contains the Python code, deep autoencoder models, model weights, and statistical results for a research project evaluating the reconstruction fidelity and prognostic clinical value of latent representations of stroke images generated by deep convolutional autoencoders.

> ⚠️ **Note**: This repository does **not** include raw clinical data due to privacy/ethics restrictions.
---

## Repository Contents

| Folder/File                           | Description                                                     |
|---------------------------------------|-----------------------------------------------------------------|
| `data_collection/`                    | Part I: collection, documentation and exploration of study data |
| `dimensionality reduction/`           | Part II: Autoencoders and linear latent transformations         |
| └──`autoencoder_utils/models`         | Autoencoder models                                              |
| └──`autoencoder_utils/outputs`        | Autoencoder weights, config logs, and trainings metrics         |
| └──`autoencoder_utils/`               | Autoencoder configs and utils                                   |
| └──`clinical_validation_nihss/`       | Prognostic evaluation for stroke severity measures              |
| └──`clinical_validation_cognition/`   | Prognostic evaluation for cognitive stroke outcome measures     |
| `statistical_analysis/`               | Part III: Main statistical analyses & visualisation                  |
| `requirements.txt`                    | Python dependencies (from `venv`)                               |
| `LICENSE`                             | MIT License                                                     |

---

## Reproducing the Analysis

This project was developed and run using Python 3.12.9 in a local `venv` environment.

### 1. Clone the repository
```bash
git clone https://github.com/ChrisSperber/lesion-image-autoencoder
cd lesion-image-autoencoder
```
### 2. Set up environment
```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```
### 3. Install dependencies
```bash
pip install -r requirements.txt
```
### 4. Run Analysis

The main analysis scripts are sequentially ordered with alphabetic prefixes. See docstrings for further information.

---

## Details on input data
All input data are stroke lesion segmentations normalised into a common space, as documented in data_collection/a_verify_and_collect_lesion_data.csv.
The current models were trained on images with a size of 79 x 95 x 79 voxels, which were preprocessed and padded by the Dataset class defined in dimensionality_reduction/autoencoder_utils/autoencoder_dataset.py

---
### Reusability
Feel free to reuse and adapt the analysis code for similar datasets. The current code assumes all images to be located in the same reference space (like MNI space) and unified in shape and resolution.

---
### References
TBA

---
### Contact
For questions, contact Christoph Sperber (see corresponding author [here](https://link.springer.com/article/10.1007/s00429-022-02559-x)).

---
### License
This project is licensed under the MIT License — see [Project License](LICENSE) for details.